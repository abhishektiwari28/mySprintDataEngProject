{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f2c6dd-6967-4937-aabe-ca865c482ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/24 13:56:33 INFO SparkContext: Running Spark version 3.5.1\n",
      "25/07/24 13:56:33 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 13:56:33 INFO SparkContext: Java version 11.0.26\n",
      "25/07/24 13:56:33 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "25/07/24 13:56:34 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 13:56:34 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/24 13:56:34 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 13:56:34 INFO SparkContext: Submitted application: Ques9\n",
      "25/07/24 13:56:34 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/24 13:56:34 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/24 13:56:34 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/24 13:56:34 INFO SecurityManager: Changing view acls to: dhsoni\n",
      "25/07/24 13:56:34 INFO SecurityManager: Changing modify acls to: dhsoni\n",
      "25/07/24 13:56:34 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/24 13:56:34 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/24 13:56:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dhsoni; groups with view permissions: EMPTY; users with modify permissions: dhsoni; groups with modify permissions: EMPTY\n",
      "25/07/24 13:56:36 INFO Utils: Successfully started service 'sparkDriver' on port 59740.\n",
      "25/07/24 13:56:36 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/24 13:56:36 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/24 13:56:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/24 13:56:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/24 13:56:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/24 13:56:36 INFO DiskBlockManager: Created local directory at C:\\Users\\dhsoni\\AppData\\Local\\Temp\\blockmgr-6703c718-079d-4701-84c7-63131bd15703\n",
      "25/07/24 13:56:36 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/24 13:56:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/24 13:56:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/24 13:56:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/24 13:56:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/07/24 13:56:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/07/24 13:56:37 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/07/24 13:56:37 INFO Utils: Successfully started service 'SparkUI' on port 4044.\n",
      "25/07/24 13:56:37 INFO Executor: Starting executor ID driver on host 10.220.71.22\n",
      "25/07/24 13:56:37 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 13:56:37 INFO Executor: Java version 11.0.26\n",
      "25/07/24 13:56:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/24 13:56:37 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5821f42d for default.\n",
      "25/07/24 13:56:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59791.\n",
      "25/07/24 13:56:37 INFO NettyBlockTransferService: Server created on 10.220.71.22:59791\n",
      "25/07/24 13:56:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/24 13:56:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.220.71.22, 59791, None)\n",
      "25/07/24 13:56:37 INFO BlockManagerMasterEndpoint: Registering block manager 10.220.71.22:59791 with 2.1 GiB RAM, BlockManagerId(driver, 10.220.71.22, 59791, None)\n",
      "25/07/24 13:56:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.220.71.22, 59791, None)\n",
      "25/07/24 13:56:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.220.71.22, 59791, None)\n",
      "25/07/24 13:56:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/24 13:56:39 INFO SharedState: Warehouse path is 'file:/C:/Users/dhsoni/spark-warehouse'.\n",
      "25/07/24 13:56:42 INFO InMemoryFileIndex: It took 144 ms to list leaf files for 1 paths.\n",
      "25/07/24 13:56:46 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/24 13:56:46 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#2),(subtype_of_property#2 = villa)\n",
      "25/07/24 13:56:49 INFO CodeGenerator: Code generated in 794.7088 ms\n",
      "25/07/24 13:56:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.220.71.22:59791 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 13:56:49 INFO SparkContext: Created broadcast 0 from show at cmd1.sc:38\n",
      "25/07/24 13:56:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Registering RDD 3 (show at cmd1.sc:38) as input to shuffle 0\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Got map stage job 0 (show at cmd1.sc:38) with 1 output partitions\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cmd1.sc:38)\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd1.sc:38), which has no missing parents\n",
      "25/07/24 13:56:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 39.8 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.220.71.22:59791 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 13:56:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 13:56:50 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd1.sc:38) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 13:56:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/24 13:56:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 13:56:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 74.1911 ms\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 24.376501 ms\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 11.853701 ms\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 19.442 ms\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 13.7751 ms\n",
      "25/07/24 13:56:51 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 13.1448 ms\n",
      "25/07/24 13:56:51 INFO CodeGenerator: Code generated in 13.6742 ms\n",
      "25/07/24 13:56:52 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2953 bytes result sent to driver\n",
      "25/07/24 13:56:52 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1398 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 13:56:52 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/24 13:56:52 INFO DAGScheduler: ShuffleMapStage 0 (show at cmd1.sc:38) finished in 1.614 s\n",
      "25/07/24 13:56:52 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 13:56:52 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 13:56:52 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 13:56:52 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 13:56:52 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 13:56:52 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 13:56:52 INFO CodeGenerator: Code generated in 47.7331 ms\n",
      "25/07/24 13:56:52 INFO SparkContext: Starting job: show at cmd1.sc:38\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Got job 1 (show at cmd1.sc:38) with 1 output partitions\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd1.sc:38)\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cmd1.sc:38), which has no missing parents\n",
      "25/07/24 13:56:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 45.3 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 2.1 GiB)\n",
      "25/07/24 13:56:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.220.71.22:59791 (size: 20.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 13:56:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cmd1.sc:38) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 13:56:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/24 13:56:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 13:56:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)\n",
      "25/07/24 13:56:52 INFO ShuffleBlockFetcherIterator: Getting 1 (138.0 B) non-empty blocks including 1 (138.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 13:56:52 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms\n",
      "25/07/24 13:56:52 INFO CodeGenerator: Code generated in 36.118199 ms\n",
      "25/07/24 13:56:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5390 bytes result sent to driver\n",
      "25/07/24 13:56:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 241 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 13:56:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/24 13:56:52 INFO DAGScheduler: ResultStage 2 (show at cmd1.sc:38) finished in 0.290 s\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 13:56:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/24 13:56:52 INFO DAGScheduler: Job 1 finished: show at cmd1.sc:38, took 0.341832 s\n",
      "25/07/24 13:56:54 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.220.71.22:59791 in memory (size: 20.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 13:56:55 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.220.71.22:59791 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 13:56:56 INFO CodeGenerator: Code generated in 23.5584 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|garden|Avg_Price|\n",
      "+------+---------+\n",
      "|   Yes| 507060.0|\n",
      "|    No| 486102.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@e893913\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "...\r\n",
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mvillaDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mresDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: int, Avg_Price: double]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: string, Avg_Price: double]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques9\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()\n",
    "\n",
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))\n",
    "\n",
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "\n",
    "val villaDf = haDF.filter($\"subtype_of_property\" === \"villa\")\n",
    "// villaDf.show()\n",
    "\n",
    "val resDf = villaDf.groupBy(\"garden\").agg(round(avg(\"price\"),0).alias(\"Avg_Price\"))\n",
    "// resDf.show()\n",
    "\n",
    "val res = resDf.withColumn(\"garden\", when(col(\"garden\") === 1, \"Yes\").when(col(\"garden\") === 0, \"No\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31c17f99-52fe-4870-b35d-d03f9434569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:17:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.220.71.22:59791 in memory (size: 34.0 KiB, free: 2.1 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@e893913"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques9\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9302beb1-bc9a-42f0-b822-d1caa6131e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b076bcd-fb18-4de2-87c2-a836d1c7dab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:18:05 INFO InMemoryFileIndex: It took 10 ms to list leaf files for 1 paths.\n",
      "25/07/24 14:18:05 INFO FileSourceStrategy: Pushed Filters: IsNotNull(subtype_of_property),EqualTo(subtype_of_property,villa)\n",
      "25/07/24 14:18:05 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(subtype_of_property#63),(subtype_of_property#63 = villa)\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.220.71.22:59791 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO SparkContext: Created broadcast 3 from show at cmd4.sc:11\n",
      "25/07/24 14:18:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Registering RDD 10 (show at cmd4.sc:11) as input to shuffle 1\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Got map stage job 2 (show at cmd4.sc:11) with 1 output partitions\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at cmd4.sc:11)\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd4.sc:11), which has no missing parents\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.8 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.220.71.22:59791 (size: 18.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd4.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:18:05 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:18:05 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:18:05 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "25/07/24 14:18:05 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:18:05 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2910 bytes result sent to driver\n",
      "25/07/24 14:18:05 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 312 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:18:05 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:18:05 INFO DAGScheduler: ShuffleMapStage 3 (show at cmd4.sc:11) finished in 0.373 s\n",
      "25/07/24 14:18:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:18:05 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:18:05 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:18:05 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:18:05 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:18:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:18:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.220.71.22:59791 in memory (size: 18.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO SparkContext: Starting job: show at cmd4.sc:11\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Got job 3 (show at cmd4.sc:11) with 1 output partitions\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Final stage: ResultStage 5 (show at cmd4.sc:11)\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at show at cmd4.sc:11), which has no missing parents\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 45.3 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.4 KiB, free 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.220.71.22:59791 (size: 20.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:18:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:18:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at show at cmd4.sc:11) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:18:05 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:18:05 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 3) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:18:05 INFO Executor: Running task 0.0 in stage 5.0 (TID 3)\n",
      "25/07/24 14:18:06 INFO ShuffleBlockFetcherIterator: Getting 1 (138.0 B) non-empty blocks including 1 (138.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:18:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/07/24 14:18:06 INFO Executor: Finished task 0.0 in stage 5.0 (TID 3). 5261 bytes result sent to driver\n",
      "25/07/24 14:18:06 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 3) in 63 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:18:06 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:18:06 INFO DAGScheduler: ResultStage 5 (show at cmd4.sc:11) finished in 0.117 s\n",
      "25/07/24 14:18:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:18:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/07/24 14:18:06 INFO DAGScheduler: Job 3 finished: show at cmd4.sc:11, took 0.131981 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+\n",
      "|garden|Avg_Price|\n",
      "+------+---------+\n",
      "|   Yes| 507060.0|\n",
      "|    No| 486102.0|\n",
      "+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mvillaDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mresDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: int, Avg_Price: double]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [garden: string, Avg_Price: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "\n",
    "val villaDf = haDF.filter($\"subtype_of_property\" === \"villa\")\n",
    "// villaDf.show()\n",
    "\n",
    "val resDf = villaDf.groupBy(\"garden\").agg(round(avg(\"price\"),0).alias(\"Avg_Price\"))\n",
    "// resDf.show()\n",
    "\n",
    "val res = resDf.withColumn(\"garden\", when(col(\"garden\") === 1, \"Yes\").when(col(\"garden\") === 0, \"No\"))\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32618a-8196-4bf4-8892-a5f3a0b10358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
