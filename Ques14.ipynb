{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343e7600-0256-4076-a4ce-949d90b3831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/24 15:26:54 INFO SparkContext: Running Spark version 3.5.1\n",
      "25/07/24 15:26:54 INFO SparkContext: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 15:26:54 INFO SparkContext: Java version 11.0.26\n",
      "25/07/24 15:26:55 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\n",
      "25/07/24 15:26:55 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 15:26:55 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/24 15:26:55 INFO ResourceUtils: ==============================================================\n",
      "25/07/24 15:26:55 INFO SparkContext: Submitted application: Ques14\n",
      "25/07/24 15:26:55 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/24 15:26:55 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/24 15:26:55 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/24 15:26:55 INFO SecurityManager: Changing view acls to: dhsoni\n",
      "25/07/24 15:26:55 INFO SecurityManager: Changing modify acls to: dhsoni\n",
      "25/07/24 15:26:55 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/24 15:26:55 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/24 15:26:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: dhsoni; groups with view permissions: EMPTY; users with modify permissions: dhsoni; groups with modify permissions: EMPTY\n",
      "25/07/24 15:26:58 INFO Utils: Successfully started service 'sparkDriver' on port 56325.\n",
      "25/07/24 15:26:58 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/24 15:26:58 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/24 15:26:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/24 15:26:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/24 15:26:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/24 15:26:58 INFO DiskBlockManager: Created local directory at C:\\Users\\dhsoni\\AppData\\Local\\Temp\\blockmgr-063392cd-7c4a-448c-9e10-78eb9853f7ae\n",
      "25/07/24 15:26:58 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/24 15:26:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/24 15:26:59 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4049. Attempting port 4050.\n",
      "25/07/24 15:26:59 WARN Utils: Service 'SparkUI' could not bind on port 4050. Attempting port 4051.\n",
      "25/07/24 15:26:59 INFO Utils: Successfully started service 'SparkUI' on port 4051.\n",
      "25/07/24 15:26:59 INFO Executor: Starting executor ID driver on host 10.220.71.22\n",
      "25/07/24 15:26:59 INFO Executor: OS info Windows 11, 10.0, amd64\n",
      "25/07/24 15:26:59 INFO Executor: Java version 11.0.26\n",
      "25/07/24 15:26:59 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/24 15:26:59 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@25d2fe41 for default.\n",
      "25/07/24 15:27:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56376.\n",
      "25/07/24 15:27:00 INFO NettyBlockTransferService: Server created on 10.220.71.22:56376\n",
      "25/07/24 15:27:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/24 15:27:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.220.71.22, 56376, None)\n",
      "25/07/24 15:27:00 INFO BlockManagerMasterEndpoint: Registering block manager 10.220.71.22:56376 with 2.1 GiB RAM, BlockManagerId(driver, 10.220.71.22, 56376, None)\n",
      "25/07/24 15:27:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.220.71.22, 56376, None)\n",
      "25/07/24 15:27:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.220.71.22, 56376, None)\n",
      "25/07/24 15:27:01 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/07/24 15:27:01 INFO SharedState: Warehouse path is 'file:/C:/Users/dhsoni/spark-warehouse'.\n",
      "25/07/24 15:27:06 INFO InMemoryFileIndex: It took 176 ms to list leaf files for 1 paths.\n",
      "25/07/24 15:27:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new)\n",
      "25/07/24 15:27:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#1),isnotnull(state_of_building#9),(type_of_property#1 = house),(state_of_building#9 = as new)\n",
      "25/07/24 15:27:14 INFO CodeGenerator: Code generated in 924.869 ms\n",
      "25/07/24 15:27:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.220.71.22:56376 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:14 INFO SparkContext: Created broadcast 0 from show at cmd1.sc:35\n",
      "25/07/24 15:27:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Registering RDD 3 (show at cmd1.sc:35) as input to shuffle 0\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Got map stage job 0 (show at cmd1.sc:35) with 1 output partitions\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cmd1.sc:35)\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd1.sc:35), which has no missing parents\n",
      "25/07/24 15:27:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 39.5 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.220.71.22:56376 (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cmd1.sc:35) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 15:27:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 132.0035 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 58.4164 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 25.3576 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 81.1657 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 76.849 ms\n",
      "25/07/24 15:27:16 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 25.532 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 17.7103 ms\n",
      "25/07/24 15:27:16 INFO CodeGenerator: Code generated in 12.6039 ms\n",
      "25/07/24 15:27:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2953 bytes result sent to driver\n",
      "25/07/24 15:27:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2287 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:18 INFO DAGScheduler: ShuffleMapStage 0 (show at cmd1.sc:35) finished in 2.570 s\n",
      "25/07/24 15:27:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 15:27:18 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 15:27:18 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 15:27:18 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 15:27:18 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 15:27:18 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 15:27:18 INFO CodeGenerator: Code generated in 49.767 ms\n",
      "25/07/24 15:27:18 INFO SparkContext: Starting job: show at cmd1.sc:35\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Got job 1 (show at cmd1.sc:35) with 1 output partitions\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Final stage: ResultStage 2 (show at cmd1.sc:35)\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cmd1.sc:35), which has no missing parents\n",
      "25/07/24 15:27:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 42.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.220.71.22:56376 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cmd1.sc:35) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 15:27:18 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)\n",
      "25/07/24 15:27:18 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 15:27:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms\n",
      "25/07/24 15:27:18 INFO CodeGenerator: Code generated in 42.7624 ms\n",
      "25/07/24 15:27:18 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 5815 bytes result sent to driver\n",
      "25/07/24 15:27:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 364 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:19 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:19 INFO DAGScheduler: ResultStage 2 (show at cmd1.sc:35) finished in 0.433 s\n",
      "25/07/24 15:27:19 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 15:27:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/07/24 15:27:19 INFO DAGScheduler: Job 1 finished: show at cmd1.sc:35, took 0.490088 s\n",
      "25/07/24 15:27:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.220.71.22:56376 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.220.71.22:56376 in memory (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:22 INFO CodeGenerator: Code generated in 28.6528 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "| subtype_of_property|Total_price_New|\n",
      "+--------------------+---------------+\n",
      "|         manor house|        5944000|\n",
      "|               villa|      479928365|\n",
      "|           farmhouse|        6437500|\n",
      "|              castle|        1799999|\n",
      "|  mixed use building|       42838711|\n",
      "|     apartment block|       58532399|\n",
      "|              chalet|        3953300|\n",
      "|            bungalow|       12327000|\n",
      "|      other property|        5257650|\n",
      "|             mansion|       57171000|\n",
      "|     country cottage|       26633900|\n",
      "|               house|     1451322529|\n",
      "|          town house|       38780409|\n",
      "|exceptional property|       93186773|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 15:27:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new),IsNotNull(subtype_of_property)\n",
      "25/07/24 15:27:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#1),isnotnull(state_of_building#9),(type_of_property#1 = house),(state_of_building#9 = as new),isnotnull(subtype_of_property#2)\n",
      "25/07/24 15:27:23 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,just renovated),IsNotNull(subtype_of_property)\n",
      "25/07/24 15:27:23 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#71),isnotnull(state_of_building#79),(type_of_property#71 = house),(state_of_building#79 = just renovated),isnotnull(subtype_of_property#72)\n",
      "25/07/24 15:27:24 INFO CodeGenerator: Code generated in 86.5308 ms\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.220.71.22:56376 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO SparkContext: Created broadcast 3 from show at cmd1.sc:56\n",
      "25/07/24 15:27:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Registering RDD 10 (show at cmd1.sc:56) as input to shuffle 1\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Got map stage job 2 (show at cmd1.sc:56) with 1 output partitions\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at cmd1.sc:56)\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd1.sc:56), which has no missing parents\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 39.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.220.71.22:56376 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cmd1.sc:56) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:24 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:24 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 15:27:24 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "25/07/24 15:27:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.220.71.22:56376 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO CodeGenerator: Code generated in 159.5994 ms\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO CodeGenerator: Code generated in 101.1489 ms\n",
      "25/07/24 15:27:24 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.220.71.22:56376 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO SparkContext: Created broadcast 5 from show at cmd1.sc:56\n",
      "25/07/24 15:27:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 15:27:24 INFO CodeGenerator: Code generated in 23.2544 ms\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Registering RDD 14 (show at cmd1.sc:56) as input to shuffle 2\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Got map stage job 3 (show at cmd1.sc:56) with 1 output partitions\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (show at cmd1.sc:56)\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[14] at show at cmd1.sc:56), which has no missing parents\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.1 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.220.71.22:56376 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:24 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[14] at show at cmd1.sc:56) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:24 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:24 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 15:27:24 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "25/07/24 15:27:24 INFO CodeGenerator: Code generated in 92.2992 ms\n",
      "25/07/24 15:27:24 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 15:27:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 2953 bytes result sent to driver\n",
      "25/07/24 15:27:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 938 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:25 INFO DAGScheduler: ShuffleMapStage 3 (show at cmd1.sc:56) finished in 0.978 s\n",
      "25/07/24 15:27:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 15:27:25 INFO DAGScheduler: running: HashSet(ShuffleMapStage 4)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 15:27:25 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 15:27:25 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 15:27:25 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 15:27:25 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2910 bytes result sent to driver\n",
      "25/07/24 15:27:25 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 742 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:25 INFO DAGScheduler: ShuffleMapStage 4 (show at cmd1.sc:56) finished in 0.779 s\n",
      "25/07/24 15:27:25 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 15:27:25 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 15:27:25 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 15:27:25 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 15:27:25 INFO CodeGenerator: Code generated in 81.8216 ms\n",
      "25/07/24 15:27:25 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Got job 4 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Final stage: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 43.1 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.220.71.22:56376 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.220.71.22:56376 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[17] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:25 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 15:27:25 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.220.71.22:56376 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "25/07/24 15:27:25 INFO CodeGenerator: Code generated in 51.947 ms\n",
      "25/07/24 15:27:25 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 5619 bytes result sent to driver\n",
      "25/07/24 15:27:25 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 114 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:25 INFO DAGScheduler: ResultStage 6 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.174 s\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Job 4 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.191831 s\n",
      "25/07/24 15:27:25 INFO CodeGenerator: Code generated in 15.602 ms\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.0 MiB, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 750.0 B, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.220.71.22:56376 (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO SparkContext: Created broadcast 8 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 15:27:25 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 15:27:25 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 15:27:25 INFO CodeGenerator: Code generated in 41.4885 ms\n",
      "25/07/24 15:27:25 INFO SparkContext: Starting job: show at cmd1.sc:56\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Got job 5 (show at cmd1.sc:56) with 1 output partitions\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Final stage: ResultStage 8 (show at cmd1.sc:56)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at show at cmd1.sc:56), which has no missing parents\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 63.2 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.220.71.22:56376 (size: 26.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:27:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.220.71.22:56376 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:27:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at show at cmd1.sc:56) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:27:25 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:27:25 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 5) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 15:27:25 INFO Executor: Running task 0.0 in stage 8.0 (TID 5)\n",
      "25/07/24 15:27:25 INFO ShuffleBlockFetcherIterator: Getting 1 (1143.0 B) non-empty blocks including 1 (1143.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 15:27:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/24 15:27:26 INFO CodeGenerator: Code generated in 65.2077 ms\n",
      "25/07/24 15:27:26 INFO Executor: Finished task 0.0 in stage 8.0 (TID 5). 8671 bytes result sent to driver\n",
      "25/07/24 15:27:26 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 5) in 156 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:27:26 INFO DAGScheduler: ResultStage 8 (show at cmd1.sc:56) finished in 0.211 s\n",
      "25/07/24 15:27:26 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:27:26 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 15:27:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "25/07/24 15:27:26 INFO DAGScheduler: Job 5 finished: show at cmd1.sc:56, took 0.242285 s\n",
      "25/07/24 15:27:26 INFO CodeGenerator: Code generated in 14.3179 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------------------+----------------+\n",
      "| subtype_of_property|Total_price_New|Total_price_Renovated|Price Difference|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "|         manor house|        5944000|              2499000|         3445000|\n",
      "|               villa|      479928365|             39234499|       440693866|\n",
      "|           farmhouse|        6437500|              2629000|         3808500|\n",
      "|  mixed use building|       42838711|             22061399|        20777312|\n",
      "|     apartment block|       58532399|             25896399|        32636000|\n",
      "|              chalet|        3953300|               284500|         3668800|\n",
      "|            bungalow|       12327000|              3535500|         8791500|\n",
      "|      other property|        5257650|              1328000|         3929650|\n",
      "|     country cottage|       26633900|             10623000|        16010900|\n",
      "|             mansion|       57171000|             15726900|        41444100|\n",
      "|               house|     1451322529|            357589108|      1093733421|\n",
      "|          town house|       38780409|              8910400|        29870009|\n",
      "|exceptional property|       93186773|             12040000|        81146773|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@62649d81\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "...\r\n",
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mhouseNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint]\r\n",
       "\u001b[36mhouseRenovated\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceRen\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_Renovated: bigint]\r\n",
       "\u001b[36mmerge2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 1 more field]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques14\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()\n",
    "\n",
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))\n",
    "\n",
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "\n",
    "val houseNew = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"as new\")\n",
    "// houseNew.show()\n",
    "\n",
    "val totPriceNew = houseNew.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_New\"))\n",
    "totPriceNew.show()\n",
    "\n",
    "\n",
    "// For Renovated\n",
    "val houseRenovated = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"just renovated\")\n",
    "// houseRenovated.show()\n",
    "\n",
    "val totPriceRen = houseRenovated.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_Renovated\"))\n",
    "// totPriceRen.show()\n",
    "\n",
    "val merge2 = (totPriceNew\n",
    "             .join(totPriceRen, Seq(\"subtype_of_property\"))\n",
    "              .select(\n",
    "                  col(\"subtype_of_property\"),\n",
    "                  col(\"Total_price_New\"),\n",
    "                  col(\"Total_price_Renovated\")\n",
    "              )\n",
    "             )\n",
    "// merge2.show()\n",
    "\n",
    "val res = merge2.withColumn(\"Price Difference\", $\"Total_price_New\" - $\"Total_price_Renovated\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326210dc-9f68-4675-85a5-def8478c2016",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 10.220.71.22:60652 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 10.220.71.22:60652 in memory (size: 26.3 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 10.220.71.22:60652 in memory (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 10.220.71.22:60652 in memory (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 10.220.71.22:60652 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 10.220.71.22:60652 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:04:50 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 10.220.71.22:60652 in memory (size: 17.9 KiB, free: 2.1 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\r\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3821a12b"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark:spark-sql_2.13:3.5.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Ques14\")\n",
    "  .master(\"local[*]\")  // Use all cores\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1baf5f4-3787-47e4-91b7-f9686770695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\r\n",
       "\r\n",
       "\u001b[39m\r\n",
       "\u001b[36mschema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"locality\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"type_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"subtype_of_property\"\u001b[39m,\r\n",
       "    dataType = StringType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"price\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"no_of_rooms\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"open_fire\"\u001b[39m,\r\n",
       "    dataType = IntegerType,\r\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\r\n",
       "    metadata = {}\r\n",
       "  ),\r\n",
       "  \u001b[33mStructField\u001b[39m(\r\n",
       "    name = \u001b[32m\"terrace\"\u001b[39m,\r\n",
       "..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create a DataFrame from a sequence of data\n",
    "import spark.implicits._\n",
    "\n",
    "val schema = StructType(Seq(\n",
    "    StructField(\"locality\", IntegerType, true),\n",
    "    StructField(\"type_of_property\", StringType, true),\n",
    "    StructField(\"subtype_of_property\", StringType, true),\n",
    "    StructField(\"price\", IntegerType, true),\n",
    "    StructField(\"no_of_rooms\", IntegerType, true),\n",
    "    StructField(\"open_fire\", IntegerType, true),\n",
    "    StructField(\"terrace\", IntegerType, true),\n",
    "    StructField(\"garden\", IntegerType, true),\n",
    "    StructField(\"swimming_pool\", IntegerType, true),\n",
    "    StructField(\"state_of_building\", StringType, true),\n",
    "    StructField(\"construction_year\", IntegerType, true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4227c5-ced7-49cc-8be5-47ccebeb23e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:14:06 INFO InMemoryFileIndex: It took 11 ms to list leaf files for 1 paths.\n",
      "25/07/24 14:14:06 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new)\n",
      "25/07/24 14:14:06 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#313),isnotnull(state_of_building#321),(type_of_property#313 = house),(state_of_building#321 = as new)\n",
      "25/07/24 14:14:06 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.220.71.22:60652 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO SparkContext: Created broadcast 22 from show at cmd5.sc:6\n",
      "25/07/24 14:14:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Registering RDD 49 (show at cmd5.sc:6) as input to shuffle 6\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Got map stage job 13 (show at cmd5.sc:6) with 1 output partitions\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (show at cmd5.sc:6)\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[49] at show at cmd5.sc:6), which has no missing parents\n",
      "25/07/24 14:14:06 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 39.5 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.220.71.22:60652 (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:06 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[49] at show at cmd5.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:06 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:06 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 13) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:14:06 INFO Executor: Running task 0.0 in stage 19.0 (TID 13)\n",
      "25/07/24 14:14:07 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:14:07 INFO Executor: Finished task 0.0 in stage 19.0 (TID 13). 2867 bytes result sent to driver\n",
      "25/07/24 14:14:07 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 13) in 405 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:07 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:07 INFO DAGScheduler: ShuffleMapStage 19 (show at cmd5.sc:6) finished in 0.422 s\n",
      "25/07/24 14:14:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:14:07 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 14:14:07 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:14:07 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:14:07 INFO ShufflePartitionsUtil: For shuffle(6), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:14:07 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:14:07 INFO SparkContext: Starting job: show at cmd5.sc:6\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Got job 14 (show at cmd5.sc:6) with 1 output partitions\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Final stage: ResultStage 21 (show at cmd5.sc:6)\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[52] at show at cmd5.sc:6), which has no missing parents\n",
      "25/07/24 14:14:07 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 42.9 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:07 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:07 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.220.71.22:60652 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:07 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[52] at show at cmd5.sc:6) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:07 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:07 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 14) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:14:07 INFO Executor: Running task 0.0 in stage 21.0 (TID 14)\n",
      "25/07/24 14:14:07 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:14:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/24 14:14:07 INFO Executor: Finished task 0.0 in stage 21.0 (TID 14). 5686 bytes result sent to driver\n",
      "25/07/24 14:14:07 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 14) in 32 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:07 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:07 INFO DAGScheduler: ResultStage 21 (show at cmd5.sc:6) finished in 0.048 s\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:14:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "25/07/24 14:14:07 INFO DAGScheduler: Job 14 finished: show at cmd5.sc:6, took 0.054502 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "| subtype_of_property|Total_price_New|\n",
      "+--------------------+---------------+\n",
      "|         manor house|        5944000|\n",
      "|               villa|      479928365|\n",
      "|           farmhouse|        6437500|\n",
      "|              castle|        1799999|\n",
      "|  mixed use building|       42838711|\n",
      "|     apartment block|       58532399|\n",
      "|              chalet|        3953300|\n",
      "|            bungalow|       12327000|\n",
      "|      other property|        5257650|\n",
      "|             mansion|       57171000|\n",
      "|     country cottage|       26633900|\n",
      "|               house|     1451322529|\n",
      "|          town house|       38780409|\n",
      "|exceptional property|       93186773|\n",
      "+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mhaDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mhouseNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceNew\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint]\r\n",
       "\u001b[36mhouseRenovated\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [locality: int, type_of_property: string ... 9 more fields]\r\n",
       "\u001b[36mtotPriceRen\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_Renovated: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val haDF = spark.read.option(\"header\",\"false\").schema(schema).csv(\"house_apartment_cleaned_data.csv\")\n",
    "// haDF.show()\n",
    "val houseNew = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"as new\")\n",
    "// houseNew.show()\n",
    "val totPriceNew = houseNew.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_New\"))\n",
    "totPriceNew.show()\n",
    "\n",
    "// For Renovated\n",
    "val houseRenovated = haDF.filter($\"type_of_property\" === \"house\" && $\"state_of_building\" === \"just renovated\")\n",
    "// houseRenovated.show()\n",
    "val totPriceRen = houseRenovated.groupBy(\"subtype_of_property\").agg(sum(\"price\").alias(\"Total_price_Renovated\"))\n",
    "// totPriceRen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "794f25e8-f5b9-4900-81cc-a68166f0bfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 14:14:07 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 10.220.71.22:60652 in memory (size: 17.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:07 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 10.220.71.22:60652 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new),IsNotNull(subtype_of_property)\n",
      "25/07/24 14:14:10 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#313),isnotnull(state_of_building#321),(type_of_property#313 = house),(state_of_building#321 = as new),isnotnull(subtype_of_property#314)\n",
      "25/07/24 14:14:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,just renovated),IsNotNull(subtype_of_property)\n",
      "25/07/24 14:14:10 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#383),isnotnull(state_of_building#391),(type_of_property#383 = house),(state_of_building#391 = just renovated),isnotnull(subtype_of_property#384)\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.220.71.22:60652 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO SparkContext: Created broadcast 25 from show at cmd6.sc:12\n",
      "25/07/24 14:14:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Registering RDD 56 (show at cmd6.sc:12) as input to shuffle 7\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Got map stage job 15 (show at cmd6.sc:12) with 1 output partitions\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Final stage: ShuffleMapStage 22 (show at cmd6.sc:12)\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Submitting ShuffleMapStage 22 (MapPartitionsRDD[56] at show at cmd6.sc:12), which has no missing parents\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 39.9 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.220.71.22:60652 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 22 (MapPartitionsRDD[56] at show at cmd6.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:10 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:10 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 15) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:14:10 INFO Executor: Running task 0.0 in stage 22.0 (TID 15)\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.220.71.22:60652 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO SparkContext: Created broadcast 27 from show at cmd6.sc:12\n",
      "25/07/24 14:14:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Registering RDD 60 (show at cmd6.sc:12) as input to shuffle 8\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Got map stage job 16 (show at cmd6.sc:12) with 1 output partitions\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (show at cmd6.sc:12)\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[60] at show at cmd6.sc:12), which has no missing parents\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 40.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.220.71.22:60652 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:10 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[60] at show at cmd6.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:10 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:10 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 16) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 14:14:10 INFO Executor: Running task 0.0 in stage 23.0 (TID 16)\n",
      "25/07/24 14:14:10 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 14:14:11 INFO Executor: Finished task 0.0 in stage 22.0 (TID 15). 2910 bytes result sent to driver\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 15) in 348 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: ShuffleMapStage 22 (show at cmd6.sc:12) finished in 0.366 s\n",
      "25/07/24 14:14:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:11 INFO DAGScheduler: running: HashSet(ShuffleMapStage 23)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:14:11 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:14:11 INFO ShufflePartitionsUtil: For shuffle(7), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:14:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:14:11 INFO Executor: Finished task 0.0 in stage 23.0 (TID 16). 2867 bytes result sent to driver\n",
      "25/07/24 14:14:11 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 14:14:11 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 10.220.71.22:60652 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 16) in 368 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:11 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 43.1 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.220.71.22:60652 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:11 INFO DAGScheduler: ShuffleMapStage 23 (show at cmd6.sc:12) finished in 0.396 s\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 17) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:14:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 14:14:11 INFO DAGScheduler: running: HashSet(ResultStage 25)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 14:14:11 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 14:14:11 INFO Executor: Running task 0.0 in stage 25.0 (TID 17)\n",
      "25/07/24 14:14:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/07/24 14:14:11 INFO Executor: Finished task 0.0 in stage 25.0 (TID 17). 5576 bytes result sent to driver\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 17) in 31 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:11 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.043 s\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.048656 s\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 8.0 MiB, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 750.0 B, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.220.71.22:60652 (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 14:14:11 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 14:14:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 14:14:11 INFO SparkContext: Starting job: show at cmd6.sc:12\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Got job 18 (show at cmd6.sc:12) with 1 output partitions\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Final stage: ResultStage 27 (show at cmd6.sc:12)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[66] at show at cmd6.sc:12), which has no missing parents\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 63.2 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 26.2 KiB, free 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.220.71.22:60652 (size: 26.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 14:14:11 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[66] at show at cmd6.sc:12) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 18) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 14:14:11 INFO Executor: Running task 0.0 in stage 27.0 (TID 18)\n",
      "25/07/24 14:14:11 INFO ShuffleBlockFetcherIterator: Getting 1 (1143.0 B) non-empty blocks including 1 (1143.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 14:14:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/24 14:14:11 INFO Executor: Finished task 0.0 in stage 27.0 (TID 18). 8628 bytes result sent to driver\n",
      "25/07/24 14:14:11 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 18) in 27 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "25/07/24 14:14:11 INFO DAGScheduler: ResultStage 27 (show at cmd6.sc:12) finished in 0.041 s\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 14:14:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "25/07/24 14:14:11 INFO DAGScheduler: Job 18 finished: show at cmd6.sc:12, took 0.049905 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------------------+----------------+\n",
      "| subtype_of_property|Total_price_New|Total_price_Renovated|Price Difference|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "|         manor house|        5944000|              2499000|         3445000|\n",
      "|               villa|      479928365|             39234499|       440693866|\n",
      "|           farmhouse|        6437500|              2629000|         3808500|\n",
      "|  mixed use building|       42838711|             22061399|        20777312|\n",
      "|     apartment block|       58532399|             25896399|        32636000|\n",
      "|              chalet|        3953300|               284500|         3668800|\n",
      "|            bungalow|       12327000|              3535500|         8791500|\n",
      "|      other property|        5257650|              1328000|         3929650|\n",
      "|     country cottage|       26633900|             10623000|        16010900|\n",
      "|             mansion|       57171000|             15726900|        41444100|\n",
      "|               house|     1451322529|            357589108|      1093733421|\n",
      "|          town house|       38780409|              8910400|        29870009|\n",
      "|exceptional property|       93186773|             12040000|        81146773|\n",
      "+--------------------+---------------+---------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmerge2\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 1 more field]\r\n",
       "\u001b[36mres\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [subtype_of_property: string, Total_price_New: bigint ... 2 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val merge2 = (totPriceNew\n",
    "             .join(totPriceRen, Seq(\"subtype_of_property\"))\n",
    "              .select(\n",
    "                  col(\"subtype_of_property\"),\n",
    "                  col(\"Total_price_New\"),\n",
    "                  col(\"Total_price_Renovated\")\n",
    "              )\n",
    "             )\n",
    "// merge2.show()\n",
    "\n",
    "val res = merge2.withColumn(\"Price Difference\", $\"Total_price_New\" - $\"Total_price_Renovated\")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdf6c1c-b9c2-49b0-9bce-8a9dd6a3ed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 15:28:00 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.220.71.22:56376 in memory (size: 26.2 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,as new),IsNotNull(subtype_of_property)\n",
      "25/07/24 15:28:04 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#1),isnotnull(state_of_building#9),(type_of_property#1 = house),(state_of_building#9 = as new),isnotnull(subtype_of_property#2)\n",
      "25/07/24 15:28:04 INFO FileSourceStrategy: Pushed Filters: IsNotNull(type_of_property),IsNotNull(state_of_building),EqualTo(type_of_property,house),EqualTo(state_of_building,just renovated),IsNotNull(subtype_of_property)\n",
      "25/07/24 15:28:04 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(type_of_property#71),isnotnull(state_of_building#79),(type_of_property#71 = house),(state_of_building#79 = just renovated),isnotnull(subtype_of_property#72)\n",
      "25/07/24 15:28:04 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.220.71.22:56376 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO SparkContext: Created broadcast 10 from orc at cmd2.sc:1\n",
      "25/07/24 15:28:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Registering RDD 24 (orc at cmd2.sc:1) as input to shuffle 3\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Got map stage job 6 (orc at cmd2.sc:1) with 1 output partitions\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (orc at cmd2.sc:1)\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[24] at orc at cmd2.sc:1), which has no missing parents\n",
      "25/07/24 15:28:04 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 39.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.220.71.22:56376 (size: 17.9 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:28:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[24] at orc at cmd2.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:28:04 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.220.71.22:56376 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:28:04 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 15:28:04 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)\n",
      "25/07/24 15:28:04 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.6 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.220.71.22:56376 in memory (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 15:28:05 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.220.71.22:56376 in memory (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.220.71.22:56376 (size: 34.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO SparkContext: Created broadcast 12 from orc at cmd2.sc:1\n",
      "25/07/24 15:28:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Registering RDD 28 (orc at cmd2.sc:1) as input to shuffle 4\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Got map stage job 7 (orc at cmd2.sc:1) with 1 output partitions\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (orc at cmd2.sc:1)\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[28] at orc at cmd2.sc:1), which has no missing parents\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 40.3 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 18.0 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.220.71.22:56376 (size: 18.0 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[28] at orc at cmd2.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:28:05 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 7) (10.220.71.22, executor driver, partition 0, PROCESS_LOCAL, 8360 bytes) \n",
      "25/07/24 15:28:05 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)\n",
      "25/07/24 15:28:05 INFO FileScanRDD: Reading File path: file:///C:/Users/dhsoni/house_apartment_cleaned_data.csv/part-00000-82b21eb4-d331-4843-a462-4c8a0c0b1b4f-c000.csv, range: 0-2565857, partition values: [empty row]\n",
      "25/07/24 15:28:05 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 2910 bytes result sent to driver\n",
      "25/07/24 15:28:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 569 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:28:05 INFO DAGScheduler: ShuffleMapStage 9 (orc at cmd2.sc:1) finished in 0.657 s\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:28:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 15:28:05 INFO DAGScheduler: running: HashSet(ShuffleMapStage 10)\n",
      "25/07/24 15:28:05 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 15:28:05 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 15:28:05 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 2867 bytes result sent to driver\n",
      "25/07/24 15:28:05 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 7) in 522 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:28:05 INFO DAGScheduler: ShuffleMapStage 10 (orc at cmd2.sc:1) finished in 0.552 s\n",
      "25/07/24 15:28:05 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/07/24 15:28:05 INFO DAGScheduler: running: HashSet()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: waiting: HashSet()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: failed: HashSet()\n",
      "25/07/24 15:28:05 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 15:28:05 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Final stage: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 43.1 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.220.71.22:56376 (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[31] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:28:05 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 8) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 15:28:05 INFO Executor: Running task 0.0 in stage 12.0 (TID 8)\n",
      "25/07/24 15:28:05 INFO ShuffleBlockFetcherIterator: Getting 1 (1151.0 B) non-empty blocks including 1 (1151.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 15:28:05 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "25/07/24 15:28:05 INFO Executor: Finished task 0.0 in stage 12.0 (TID 8). 5619 bytes result sent to driver\n",
      "25/07/24 15:28:05 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 8) in 88 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:28:05 INFO DAGScheduler: ResultStage 12 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0.155 s\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 15:28:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "25/07/24 15:28:05 INFO DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0.193308 s\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 8.0 MiB, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 750.0 B, free 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.220.71.22:56376 (size: 750.0 B, free: 2.1 GiB)\n",
      "25/07/24 15:28:05 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "25/07/24 15:28:05 INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/07/24 15:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/07/24 15:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/07/24 15:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/07/24 15:28:06 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "25/07/24 15:28:06 INFO CodeGenerator: Code generated in 34.3458 ms\n",
      "25/07/24 15:28:06 INFO SparkContext: Starting job: orc at cmd2.sc:1\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Got job 9 (orc at cmd2.sc:1) with 1 output partitions\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Final stage: ResultStage 14 (orc at cmd2.sc:1)\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[34] at orc at cmd2.sc:1), which has no missing parents\n",
      "25/07/24 15:28:06 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 260.8 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:06 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 95.8 KiB, free 2.1 GiB)\n",
      "25/07/24 15:28:06 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.220.71.22:56376 (size: 95.8 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:06 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1585\n",
      "25/07/24 15:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[34] at orc at cmd2.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "25/07/24 15:28:06 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "25/07/24 15:28:06 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 9) (10.220.71.22, executor driver, partition 0, NODE_LOCAL, 7695 bytes) \n",
      "25/07/24 15:28:06 INFO Executor: Running task 0.0 in stage 14.0 (TID 9)\n",
      "25/07/24 15:28:06 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 10.220.71.22:56376 in memory (size: 19.4 KiB, free: 2.1 GiB)\n",
      "25/07/24 15:28:06 INFO ShuffleBlockFetcherIterator: Getting 1 (1143.0 B) non-empty blocks including 1 (1143.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/07/24 15:28:06 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/07/24 15:28:06 INFO CodeGenerator: Code generated in 34.0487 ms\n",
      "25/07/24 15:28:06 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "25/07/24 15:28:06 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/07/24 15:28:06 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n",
      "25/07/24 15:28:06 INFO HadoopShimsCurrent: Can't get KeyProvider for ORC encryption from hadoop.security.key.provider.path.\n",
      "25/07/24 15:28:06 INFO PhysicalFsWriter: ORC writer created for path: file:/C:/Users/dhsoni/Results/Res14.orc/_temporary/0/_temporary/attempt_202507241528063131881312402889358_0014_m_000000_9/part-00000-69ac4ad6-ad0e-457c-bde6-44a2e378d0a7-c000.snappy.orc with stripeSize: 67108864 blockSize: 268435456 compression: Compress: SNAPPY buffer: 262144\n",
      "25/07/24 15:28:07 INFO WriterImpl: ORC writer created for path: file:/C:/Users/dhsoni/Results/Res14.orc/_temporary/0/_temporary/attempt_202507241528063131881312402889358_0014_m_000000_9/part-00000-69ac4ad6-ad0e-457c-bde6-44a2e378d0a7-c000.snappy.orc with stripeSize: 67108864 options: Compress: SNAPPY buffer: 262144\n",
      "25/07/24 15:28:07 INFO FileOutputCommitter: Saved output of task 'attempt_202507241528063131881312402889358_0014_m_000000_9' to file:/C:/Users/dhsoni/Results/Res14.orc/_temporary/0/task_202507241528063131881312402889358_0014_m_000000\n",
      "25/07/24 15:28:07 INFO SparkHadoopMapRedUtil: attempt_202507241528063131881312402889358_0014_m_000000_9: Committed. Elapsed time: 16 ms.\n",
      "25/07/24 15:28:07 INFO Executor: Finished task 0.0 in stage 14.0 (TID 9). 9183 bytes result sent to driver\n",
      "25/07/24 15:28:07 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 9) in 1161 ms on 10.220.71.22 (executor driver) (1/1)\n",
      "25/07/24 15:28:07 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "25/07/24 15:28:07 INFO DAGScheduler: ResultStage 14 (orc at cmd2.sc:1) finished in 1.271 s\n",
      "25/07/24 15:28:07 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/07/24 15:28:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "25/07/24 15:28:07 INFO DAGScheduler: Job 9 finished: orc at cmd2.sc:1, took 1.285080 s\n",
      "25/07/24 15:28:07 INFO FileFormatWriter: Start to commit write Job b4e16d0c-7e51-483c-9e0e-84e961907d3e.\n",
      "25/07/24 15:28:07 INFO FileFormatWriter: Write Job b4e16d0c-7e51-483c-9e0e-84e961907d3e committed. Elapsed time: 73 ms.\n",
      "25/07/24 15:28:07 INFO FileFormatWriter: Finished processing stats for write job b4e16d0c-7e51-483c-9e0e-84e961907d3e.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Saved as orc"
     ]
    }
   ],
   "source": [
    "res.write.orc(\"Results/Res14.orc\")\n",
    "print(\"File Saved as orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d748e-e700-4865-8684-a2ca69dcf688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
